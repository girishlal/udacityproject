--- /opt/conda/lib/python3.6/site-packages/torch/nn/modules/activation.py
+++ /opt/conda/lib/python3.6/site-packages/torch/nn/modules/activation.py
@@ -1,17 +1,16 @@
 class LogSoftmax(Module):
-    r"""Applies the :math:`\log(\text{Softmax}(x))` function to an n-dimensional
-    input Tensor. The LogSoftmax formulation can be simplified as:
+    r"""Applies the `Log(Softmax(x))` function to an n-dimensional input Tensor.
+    The LogSoftmax formulation can be simplified as
 
-    .. math::
-        \text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)
+    :math:`\text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)`
 
     Shape:
-        - Input: :math:`(*)` where `*` means, any number of additional
-          dimensions
-        - Output: :math:`(*)`, same shape as the input
+        - Input: any shape
+        - Output: same as input
 
     Arguments:
-        dim (int): A dimension along which LogSoftmax will be computed.
+        dim (int): A dimension along which Softmax will be computed (so every slice
+            along dim will sum to 1).
 
     Returns:
         a Tensor of the same dimension and shape as the input with
@@ -23,7 +22,6 @@
         >>> input = torch.randn(2, 3)
         >>> output = m(input)
     """
-    __constants__ = ['dim']
 
     def __init__(self, dim=None):
         super(LogSoftmax, self).__init__()